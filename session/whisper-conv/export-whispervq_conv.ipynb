{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7997ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732e4d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f6f03fbf700>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from whispervq_conv import WhisperForConditionalGeneration\n",
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3c2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "WhisperForConditionalGeneration.register_for_auto_class(\"AutoModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3191f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-279000  checkpoint-279500  checkpoint-280000  runs\r\n"
     ]
    }
   ],
   "source": [
    "!ls whisper-conv-large-v3-turbo-vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ace0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    'whisper-conv-large-v3-turbo-vq/checkpoint-280000', torch_dtype = 'auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9113662",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "636c87c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained('openai/whisper-large-v3-turbo')\n",
    "tokenizer = AutoTokenizer.from_pretrained('openai/whisper-large-v3-turbo')\n",
    "processor = AutoProcessor.from_pretrained('openai/whisper-large-v3-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f6c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import MDSWriter, LocalDataset\n",
    "\n",
    "dataset = LocalDataset('mosaic-stt-include-malaysian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4a896b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('audio/fr/dev/audio/common_voice_fr_19722301.mp3',\n",
       " 'audio/fr/dev/audio/common_voice_fr_19722302.mp3')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-100003]['audio_filename'], dataset[-100002]['audio_filename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3b0893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = model.model.get_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d38725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `WhisperFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([14135,  7585, 12890, 32383, 15559,  4515,   252, 32713,   252, 16296,\n",
       "         3050, 18175, 15733,  5619,  5619,  1770,  7520, 32041, 26287,  8139,\n",
       "         8453, 28652,  4327, 26837, 20927, 26620, 12310, 12310, 12938, 29755,\n",
       "        29755, 18102,  5597,  8076,  8076,  8076,  9772, 31738, 31738,  1856,\n",
       "        24397, 27124,  5538,  1970, 29984,  8891, 20453, 20453,  1815,  1465,\n",
       "         1465, 26893,  5597,  9531, 11871, 11871,  6484, 21016, 14653, 18417,\n",
       "         9598,  9598, 30138, 27531, 18071, 18071, 30147, 24892,   434, 16557,\n",
       "        30589, 25516, 30876, 30876, 32039, 29394, 27996, 10042,  1939, 16692,\n",
       "         8163, 16665, 16665,  4507, 28100, 31251,  3051,  3051, 12157, 19865,\n",
       "        27147, 27357, 21524, 19750, 20016,  9031, 20016, 13475, 30149, 30149,\n",
       "        21785,  4176, 24032, 19334, 17387, 31375,  2659, 16509, 31672,  7785,\n",
       "        10352, 30063,  8518, 30730, 29357, 28538,  7072], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, sr = librosa.load('common_voice_ba_26517811.mp3', sr = feature_extractor.sampling_rate)\n",
    "features = feature_extractor([y], return_tensors = 'pt', return_attention_mask = True)\n",
    "for k in features.keys():\n",
    "    features[k] = features[k].cuda()\n",
    "encoded = encoder(**features)\n",
    "encoded[1][0, encoded[2][0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57592a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `WhisperFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Hey, I got some questions about my vaccination schedule lah. Last time jab first dose Moderna, second dose Pfizer. Now how to continue to book?<|endoftext|>',\n",
       " '<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Ehhh, I got some question about my vaccination schedule la. Last time jab first dose Moderna, second dose Pfizer. Now how to continue with booster?<|endoftext|>')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 100\n",
    "t = dataset[i]['text']\n",
    "y, sr = librosa.load(dataset[i]['audio_filename'], sr = feature_extractor.sampling_rate)\n",
    "tokenized = tokenizer(dataset[i]['text'], add_special_tokens = False)['input_ids']\n",
    "input_ids = []\n",
    "for k in tokenized:\n",
    "    input_ids.append(k)\n",
    "    if k == 50364:\n",
    "        break\n",
    "features = feature_extractor([y], return_tensors = 'pt', return_attention_mask = True)\n",
    "features['decoder_input_ids'] = torch.tensor(input_ids)[None]\n",
    "for k in features.keys():\n",
    "    features[k] = features[k].cuda()\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    **features,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.0,\n",
    "    do_sample=False\n",
    ")\n",
    "generation_output = model.generate(**generate_kwargs)\n",
    "tokenizer.decode(generation_output[0]), t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9828864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the `sampling_rate` argument to `WhisperFeatureExtractor()`. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|startoftranscript|><|ru|><|transcribe|><|notimestamps|> Кубах сирта был холква кешене битарафлыг сирпаса.<|endoftext|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, sr = librosa.load('common_voice_ba_26517811.mp3', sr = feature_extractor.sampling_rate)\n",
    "input_ids = tokenizer(\n",
    "    '<|startoftranscript|><|ru|><|transcribe|><|notimestamps|>', \n",
    "    add_special_tokens = False, return_tensors = 'pt')['input_ids']\n",
    "features = feature_extractor([y], return_tensors = 'pt', return_attention_mask = True)\n",
    "features['decoder_input_ids'] = input_ids\n",
    "for k in features.keys():\n",
    "    features[k] = features[k].cuda()\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    **features,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "generation_output = model.generate(**generate_kwargs)\n",
    "tokenizer.decode(generation_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ae330",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('218757.mp3', sr = feature_extractor.sampling_rate)\n",
    "input_ids = tokenizer(\n",
    "    '<|startoftranscript|><|ms|><|transcribe|><|notimestamps|>', \n",
    "    add_special_tokens = False, return_tensors = 'pt')['input_ids']\n",
    "features = feature_extractor([y], return_tensors = 'pt', return_attention_mask = True)\n",
    "features['decoder_input_ids'] = input_ids\n",
    "for k in features.keys():\n",
    "    features[k] = features[k].cuda()\n",
    "\n",
    "generate_kwargs = dict(\n",
    "    **features,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.1,\n",
    "    do_sample=True\n",
    ")\n",
    "generation_output = model.generate(**generate_kwargs)\n",
    "tokenizer.decode(generation_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f4036e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9a829983bb4cd796ac565861dd0ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-05 08:39:08,343] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/usr/bin/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a47aba7cf54d72a7a4be5f8909d153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading...:   0%|          | 0.00/3.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/whisper-conv-VQ-32k-large-v3-turbo/commit/23bea549fbe1f3533d7a05ae4f056b0e2f7441af', commit_message='Upload WhisperForConditionalGeneration', commit_description='', oid='23bea549fbe1f3533d7a05ae4f056b0e2f7441af', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mesolitica/whisper-conv-VQ-32k-large-v3-turbo', endpoint='https://huggingface.co', repo_type='model', repo_id='mesolitica/whisper-conv-VQ-32k-large-v3-turbo'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('mesolitica/whisper-conv-VQ-32k-large-v3-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a7eaf07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/whisper-conv-VQ-32k-large-v3-turbo/commit/d10350d1730ad542954800825dc817ddcfa3a05d', commit_message='Upload tokenizer', commit_description='', oid='d10350d1730ad542954800825dc817ddcfa3a05d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mesolitica/whisper-conv-VQ-32k-large-v3-turbo', endpoint='https://huggingface.co', repo_type='model', repo_id='mesolitica/whisper-conv-VQ-32k-large-v3-turbo'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('mesolitica/whisper-conv-VQ-32k-large-v3-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f3a01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/whisper-conv-VQ-32k-large-v3-turbo/commit/d10350d1730ad542954800825dc817ddcfa3a05d', commit_message='Upload feature extractor', commit_description='', oid='d10350d1730ad542954800825dc817ddcfa3a05d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mesolitica/whisper-conv-VQ-32k-large-v3-turbo', endpoint='https://huggingface.co', repo_type='model', repo_id='mesolitica/whisper-conv-VQ-32k-large-v3-turbo'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.push_to_hub('mesolitica/whisper-conv-VQ-32k-large-v3-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17959453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/mesolitica/whisper-conv-VQ-32k-large-v3-turbo/commit/901d4247bbb4eaa57fa7a202fee9ed7a8ea45f63', commit_message='Upload processor', commit_description='', oid='901d4247bbb4eaa57fa7a202fee9ed7a8ea45f63', pr_url=None, repo_url=RepoUrl('https://huggingface.co/mesolitica/whisper-conv-VQ-32k-large-v3-turbo', endpoint='https://huggingface.co', repo_type='model', repo_id='mesolitica/whisper-conv-VQ-32k-large-v3-turbo'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.push_to_hub('mesolitica/whisper-conv-VQ-32k-large-v3-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82d110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
