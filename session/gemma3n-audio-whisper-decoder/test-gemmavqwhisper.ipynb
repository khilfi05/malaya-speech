{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7997ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02dd092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f6e01bc1420>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gemmavqwhisper import GemmaWhisperForConditionalGeneration, Model\n",
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "import librosa\n",
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c12b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "import os\n",
    "\n",
    "def init_distributed_single():\n",
    "    if not dist.is_initialized():\n",
    "        dist.init_process_group(\n",
    "            backend=\"nccl\" if torch.cuda.is_available() else \"gloo\",\n",
    "            init_method=\"tcp://127.0.0.1:23456\",\n",
    "            rank=0,\n",
    "            world_size=1\n",
    "        )\n",
    "\n",
    "init_distributed_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0ace0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Model were not initialized from the model checkpoint at mesolitica/gemma3n-audio-encoder-whisper-decoder and are newly initialized: ['model.codebook.weight', 'model.ema_count', 'model.ema_weight', 'model.post_layer_norm.bias', 'model.post_layer_norm.weight', 'model.post_projection.bias', 'model.post_projection.weight', 'model.total_code_usage']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded codebook weight from centroids-v2.npy\n"
     ]
    }
   ],
   "source": [
    "model = Model.from_pretrained(\n",
    "    'mesolitica/gemma3n-audio-encoder-whisper-decoder', torch_dtype = 'auto').train()\n",
    "model.model.init_quantize_layer('centroids-v2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53d53d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "same = (model.proj_out.weight == model.model.decoder.embed_tokens.weight).float().mean().tolist()\n",
    "assert same >= 0.99, \"projection is not tied\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "995bddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.6397)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.codebook.weight.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a6c5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.quantize_restart_interval = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9113662",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636c87c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    'mesolitica/gemma3n-audio-encoder-whisper-decoder')\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/gemma3n-audio-encoder-whisper-decoder')\n",
    "processor = AutoProcessor.from_pretrained('openai/whisper-large-v3-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95f6c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from streaming import MDSWriter, LocalDataset\n",
    "\n",
    "dataset = LocalDataset('mosaic-stt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b24e8c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([27024,   341,  3275, 15559, 29834, 11490,  6436, 18571, 29371, 12893,\n",
      "        26248, 31135, 21771, 23693,  9813, 28774, 11253,  7236, 32728, 20986,\n",
      "        26864, 20810, 29694, 29440, 26354, 12950,  1350, 27814, 30059, 30489,\n",
      "        11353, 29320, 16193, 13303,   192, 28362, 13868, 22161, 22161, 16128,\n",
      "         7744], device='cuda:0')\n",
      "num_update 0\n",
      "0 tensor(0.2492, device='cuda:0') 39.636878967285156 40\n",
      "0 40.0\n",
      "tensor([  597, 11259, 17979, 27277, 23815, 30235, 30235, 26312, 16543, 21771,\n",
      "        20026, 22545,  4537,   240, 30159,  6588, 28135, 24756,  7713, 11766,\n",
      "        21143,  6532, 14495, 31286, 10918], device='cuda:0')\n",
      "num_update 32743\n",
      "[VQ] Restarted 32743 dead codes.\n",
      "1 tensor(0.2525, device='cuda:0') 23.651439666748047 24\n",
      "1 63.0\n",
      "tensor([  597, 11259, 17979, 27277, 27277, 30235, 30235, 30235, 30235, 30235,\n",
      "        30235, 26312, 26312, 30235, 26312, 26312, 16543, 30159, 30159, 28135,\n",
      "        21771,  7713, 11766, 22161, 10918, 21771, 22545,   240, 24756,   597,\n",
      "        28135,   240,   597,   597, 22545,   240,  7713,  7713, 14495, 28135,\n",
      "          597,   597,  4537,   240,   597, 30159,  6588,  7713, 22161],\n",
      "       device='cuda:0')\n",
      "num_update 5\n",
      "[VQ] Restarted 5 dead codes.\n",
      "2 tensor(0.3731, device='cuda:0') 15.621600151062012 20\n",
      "2 63.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    y, sr = librosa.load(dataset[i]['audio_filename'], sr = feature_extractor.sampling_rate)\n",
    "    features = feature_extractor([y], return_tensors = 'pt')\n",
    "    features['input_features'] = features['input_features'].cuda()\n",
    "    features['input_features_mask'] = features['input_features_mask'].cuda()\n",
    "    features['attention_mask'] = features.pop('input_features_mask')\n",
    "    input_ids = tokenizer('<|startoftranscript|>' + dataset[i]['text'] + tokenizer.eos_token, \n",
    "        add_special_tokens = False, return_tensors = 'pt')['input_ids'].cuda()\n",
    "    features['labels'] = input_ids[:,1:]\n",
    "    features['decoder_input_ids'] = input_ids[:,:-1]\n",
    "    o = model(**features, use_cache = False)\n",
    "    print(i, model.model.quantize_loss, model.model.quantize_perplexity, model.model.num_active_codes)\n",
    "    print(i, model.model.total_code_usage.sum().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10",
   "language": "python",
   "name": "python3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
